{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 4229472,
          "sourceType": "datasetVersion",
          "datasetId": 2492800
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Detecting Stages of Alzheimer's Diseaseâ€‹ (96% acc)",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabianVenom/CMP1903M-A0-2223/blob/main/Detecting_Stages_of_Alzheimer's_Disease%E2%80%8B_(96_acc).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "uraninjo_augmented_alzheimer_mri_dataset_path = kagglehub.dataset_download('uraninjo/augmented-alzheimer-mri-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yesjjYNebtdi",
        "outputId": "e18e9aff-4a1f-4852-8e78-2e10ef00da61"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Important Notes\n",
        "This project was developed as part of our Medical Pattern Recognition course at Faculty of Engineering Helwan University, Biomedical Engineering Departement.\n",
        "The Model which have the highest accuracy (96%) is the final model which is: One VS One Approach"
      ],
      "metadata": {
        "id": "eEyyUK9jbtdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "AlNE6PIFbtdo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import gc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import (Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization)\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "notBg4Fdbtdo"
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Explore Data"
      ],
      "metadata": {
        "id": "xHDYjn8Qbtdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset\"\n",
        "\n",
        "data = []\n",
        "\n",
        "for category in os.listdir(dataset_path):\n",
        "    category_path = os.path.join(dataset_path, category)\n",
        "\n",
        "    if os.path.isdir(category_path):\n",
        "        for image_name in os.listdir(category_path):\n",
        "            image_path = os.path.join(category_path, image_name)\n",
        "\n",
        "            data.append({\"image_path\": image_path, \"category\": category})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "output_csv = \"dataset.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"CSV file saved as {output_csv}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKAqT2tubtdu",
        "outputId": "c74c75a4-b443-4848-c3e3-94be5d7cb572"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file saved as dataset.csv\n"
          ]
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "pCnvzZHubtdy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "category_counts = df[\"category\"].value_counts()\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.pie(category_counts, labels=category_counts.index, autopct=\"%1.1f%%\")\n",
        "plt.title(\"Category Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "szlmyjVvbtdy"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def show_random_images(column, number_of_images_to_show=5, grey=False):\n",
        "    categories = df[\"category\"].unique()\n",
        "\n",
        "    images_per_category = number_of_images_to_show\n",
        "\n",
        "    plt.figure(figsize=(15, len(categories) * 5))\n",
        "    for i, category in enumerate(categories):\n",
        "        category_images = df[df[\"category\"] == category][column].tolist()\n",
        "\n",
        "        for j in range(min(len(category_images), images_per_category)):\n",
        "            img_data = category_images[j]\n",
        "            if column != \"image_path\":\n",
        "                img = img_data\n",
        "            else:\n",
        "                img = cv2.imread(img_data)\n",
        "\n",
        "            if column == \"image_path\":\n",
        "                if grey:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "                else:\n",
        "                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            plt.subplot(len(categories), images_per_category, i * images_per_category + j + 1)\n",
        "\n",
        "            if grey:\n",
        "                plt.imshow(img, cmap=\"gray\")\n",
        "            else:\n",
        "                plt.imshow(img)\n",
        "\n",
        "            plt.axis(\"off\")\n",
        "            plt.title(f\"{category}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "8uOlAvRVbtdz"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_images(\"image_path\", 5, False)"
      ],
      "metadata": {
        "id": "7mvaXx1mbtd0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_images(\"image_path\",5, True)"
      ],
      "metadata": {
        "id": "i7qvPCJpbtd0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Processing (not the final version it will be edited later for every method)"
      ],
      "metadata": {
        "id": "D97ivvrlbtd0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_dim = cv2.imread(df[\"image_path\"][0])\n",
        "min_height, min_width = min_dim.shape[:2]\n",
        "\n",
        "for image_path in df[\"image_path\"]:\n",
        "    img = cv2.imread(image_path)\n",
        "    height, width = img.shape[:2]\n",
        "    min_height = min(min_height, height)\n",
        "    min_width = min(min_width, width)\n",
        "print(min_height)\n",
        "print(min_width)\n",
        ""
      ],
      "metadata": {
        "id": "rhVIiuaNbtd1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "processed_images = []\n",
        "\n",
        "for img_path in df[\"image_path\"]:\n",
        "    img = cv2.imread(img_path)\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    img_resized = cv2.resize(img_gray, (180,180))\n",
        "\n",
        "    processed_images.append(img_resized)\n",
        "\n",
        "df[\"resized_image\"] = processed_images\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "UI2_Tobkbtd1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "equlaize_hist = []\n",
        "clahe_hist = []\n",
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(5,5))\n",
        "\n",
        "for img in df[\"resized_image\"]:\n",
        "    img_equalized = cv2.equalizeHist(img)\n",
        "    equlaize_hist.append(img_equalized)\n",
        "    img_clahe = clahe.apply(img)\n",
        "    clahe_hist.append(img_clahe)\n",
        "\n",
        "df[\"equlaize_hist\"] = equlaize_hist\n",
        "df[\"clahe_hist\"] = clahe_hist\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "M50zGB-tbtd1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_images(\"equlaize_hist\",5, True)"
      ],
      "metadata": {
        "id": "HNq8z74wbtd1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_images(\"clahe_hist\",5, True)"
      ],
      "metadata": {
        "id": "qq4HZcUTbtd1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gblur_images = []\n",
        "median_images = []\n",
        "bilat_images = []\n",
        "\n",
        "\n",
        "for img in df[\"clahe_hist\"]:\n",
        "\n",
        "    gblur = cv2.GaussianBlur(img, (5, 5), 0)\n",
        "    gblur_images.append(gblur)\n",
        "\n",
        "    median = cv2.medianBlur(img, 5)\n",
        "    median_images.append(median)\n",
        "\n",
        "    biF = cv2.bilateralFilter(img, 5, 21, 21)\n",
        "    bilat_images.append(biF)\n",
        "\n",
        "df[\"gblur_images\"] = gblur_images\n",
        "df[\"median_images\"] = median_images\n",
        "df[\"bilat_images\"] = bilat_images\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "6cmd09Wsbtd1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_images(\"gblur_images\",5, True)"
      ],
      "metadata": {
        "id": "DM55bms4btd1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_images(\"median_images\",5, True)"
      ],
      "metadata": {
        "id": "gmJURyFtbtd2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_images(\"bilat_images\",5, True)"
      ],
      "metadata": {
        "id": "R5oi57qxbtd2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1- First Method: Using Resnet 152 Model for classification (Accuracy 73%)"
      ],
      "metadata": {
        "id": "iLPoTCN5btd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset\"\n",
        "\n",
        "data = []\n",
        "\n",
        "for category in os.listdir(dataset_path):\n",
        "    category_path = os.path.join(dataset_path, category)\n",
        "\n",
        "    if os.path.isdir(category_path):\n",
        "        for image_name in os.listdir(category_path):\n",
        "            image_path = os.path.join(category_path, image_name)\n",
        "\n",
        "            data.append({\"image_path\": image_path, \"category\": category})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "output_csv = \"dataset.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"CSV file saved as {output_csv}\")\n"
      ],
      "metadata": {
        "id": "u0_zjzC1btd2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore Data"
      ],
      "metadata": {
        "id": "EK8VNiLhbtd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_counts = df[\"category\"].value_counts()\n",
        "print(category_counts)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.pie(category_counts, labels=category_counts.index, autopct=\"%1.1f%%\")\n",
        "plt.title(\"Category Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2GN4368Bbtd2"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply UnderSampling"
      ],
      "metadata": {
        "id": "2qDh5-Xdbtd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_class_size = df['category'].value_counts().min()\n",
        "undersampled_df = (\n",
        "    df.groupby('category', group_keys=False)\n",
        "    .apply(lambda x: x.sample(n=min_class_size, random_state=42))\n",
        ")\n",
        "\n",
        "undersampled_csv = \"undersampled_dataset.csv\"\n",
        "undersampled_df.to_csv(undersampled_csv, index=False)\n",
        "\n",
        "print(\"Dataset undersampled successfully! New dataset saved to:\", undersampled_csv)\n",
        "df = undersampled_df\n",
        "del undersampled_df\n",
        "df.head()"
      ],
      "metadata": {
        "id": "OSONU6Embtd3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "category_counts = df[\"category\"].value_counts()\n",
        "print(category_counts)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.pie(category_counts, labels=category_counts.index, autopct=\"%1.1f%%\")\n",
        "plt.title(\"Category Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ieWGWbZVbtd4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Processing"
      ],
      "metadata": {
        "id": "t81Cf0d5btd4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(7,7))\n",
        "processed_images = []\n",
        "\n",
        "for img_path in df[\"image_path\"]:\n",
        "    img = cv2.imread(img_path)\n",
        "\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    img_resized = cv2.resize(img_gray, (224,224))\n",
        "\n",
        "    img_clahe = clahe.apply(img_resized)\n",
        "\n",
        "    biF = cv2.bilateralFilter(img_clahe, 5, 21, 21)\n",
        "\n",
        "    norm_img = cv2.normalize(biF, None, alpha=0.0, beta=1.0, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "\n",
        "    three_channel_image = cv2.cvtColor(norm_img, cv2.COLOR_GRAY2BGR)\n",
        "\n",
        "    processed_images.append(three_channel_image)\n",
        "\n",
        "df[\"processed_images\"] = processed_images\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "hBFtE7TZbtd4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_images(\"processed_images\",5, True)"
      ],
      "metadata": {
        "id": "h-dRtEw5btd4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply One hot encoding"
      ],
      "metadata": {
        "id": "A-_ShGqZbtd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_encoded = pd.get_dummies(df['category'], dtype=int)\n",
        "df_ohe = pd.concat([df, df_encoded], axis=1)\n",
        "df = df_ohe.drop(columns=['category', 'image_path'])\n",
        "del df_ohe, df_encoded\n",
        "gc.collect()\n",
        "df.head()"
      ],
      "metadata": {
        "id": "WxKRu4IYbtd5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Testing"
      ],
      "metadata": {
        "id": "GE97-6R9btd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "del df\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "7UfCQjcDbtd6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(df, batch_size):\n",
        "    num_samples = len(df)\n",
        "    while True:\n",
        "        for offset in range(0, num_samples, batch_size):\n",
        "            batch_df = df.iloc[offset:offset+batch_size]\n",
        "\n",
        "            X_batch = np.array(batch_df['processed_images'].tolist())\n",
        "            Y_batch = batch_df.drop(columns=['processed_images']).values\n",
        "\n",
        "            yield X_batch, Y_batch"
      ],
      "metadata": {
        "id": "L-o7pkBhbtd6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "X_iWFIzzbtd6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "predictions = Dense(4, activation='softmax')(x)\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "O7yz6K8Jbtd6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=10,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "5mq9J8mRbtd6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_gen = data_generator(train_df, batch_size=32)\n",
        "val_gen = data_generator(val_df, batch_size=32)\n",
        "steps_per_epoch = len(train_df) // 32\n",
        "validation_steps = len(val_df) // 32\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=validation_steps,\n",
        "    epochs=2000,\n",
        "    callbacks=[early_stopping]\n",
        ")"
      ],
      "metadata": {
        "id": "lM2JTPwGbtd-"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Confusion Matrix and Classification Report"
      ],
      "metadata": {
        "id": "kbre1l6pbtd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "y_test = test_df[['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']].values\n",
        "y_pred = model.predict(np.array(test_df['processed_images'].tolist()))\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "cm = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y_true), yticklabels=np.unique(y_true))\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YAhf3hRlbtd_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "report = classification_report(y_true, y_pred_classes, target_names=['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented'])\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "metadata": {
        "id": "rLyEnpqWbtd_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('Resnet_152_Model.h5')"
      ],
      "metadata": {
        "id": "Q7W5phFNbtd_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2- Second Method: Using Custom Multi Class Classification Model (Accuracy 89%)"
      ],
      "metadata": {
        "id": "EkzewlJFbtd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset\"\n",
        "\n",
        "data = []\n",
        "\n",
        "for category in os.listdir(dataset_path):\n",
        "    category_path = os.path.join(dataset_path, category)\n",
        "\n",
        "    if os.path.isdir(category_path):\n",
        "        for image_name in os.listdir(category_path):\n",
        "            image_path = os.path.join(category_path, image_name)\n",
        "\n",
        "            data.append({\"image_path\": image_path, \"category\": category})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "output_csv = \"dataset.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"CSV file saved as {output_csv}\")\n"
      ],
      "metadata": {
        "id": "XOYqyiYdbtd_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explore Data"
      ],
      "metadata": {
        "id": "N8_E4SQkbtd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "category_counts = df[\"category\"].value_counts()\n",
        "print(category_counts)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.pie(category_counts, labels=category_counts.index, autopct=\"%1.1f%%\")\n",
        "plt.title(\"Category Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WMaqxyi-bteA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply UnderSampling"
      ],
      "metadata": {
        "id": "a6Zh2WIGbteA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_class_size = df['category'].value_counts().min()\n",
        "undersampled_df = (\n",
        "    df.groupby('category', group_keys=False)\n",
        "    .apply(lambda x: x.sample(n=min_class_size, random_state=42))\n",
        ")\n",
        "\n",
        "undersampled_csv = \"undersampled_dataset.csv\"\n",
        "undersampled_df.to_csv(undersampled_csv, index=False)\n",
        "\n",
        "print(\"Dataset undersampled successfully! New dataset saved to:\", undersampled_csv)\n",
        "df = undersampled_df\n",
        "del undersampled_df\n",
        "df.head()"
      ],
      "metadata": {
        "id": "08gj3lTsbteA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "category_counts = df[\"category\"].value_counts()\n",
        "print(category_counts)\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.pie(category_counts, labels=category_counts.index, autopct=\"%1.1f%%\")\n",
        "plt.title(\"Category Distribution\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "D-di3yxPbteA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Processing"
      ],
      "metadata": {
        "id": "wox29wcabteA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_processing(img):\n",
        "    img = cv2.imread(img)\n",
        "\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    img_resized = cv2.resize(img_gray, (224,224))\n",
        "\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(7,7))\n",
        "    img_clahe = clahe.apply(img_resized)\n",
        "\n",
        "    biF = cv2.bilateralFilter(img_clahe, 5, 21, 21)\n",
        "\n",
        "    norm_img = cv2.normalize(biF, None, alpha=0.0, beta=1.0, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "\n",
        "    return norm_img"
      ],
      "metadata": {
        "id": "zdvmSiDCbteA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model"
      ],
      "metadata": {
        "id": "llu8sHXvbteB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(224, 224, 1)))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(filters=256, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "2hXMg-svbteB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = Adam(learning_rate = 0.00001), loss = 'categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "qIwtqEtIbteB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True\n",
        ")"
      ],
      "metadata": {
        "id": "kwcNye6XbteB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Data"
      ],
      "metadata": {
        "id": "cx08wz3pbteB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ohe(df):\n",
        "    df_encoded = pd.get_dummies(df['category'], dtype=int)\n",
        "    df_ohe = pd.concat([df, df_encoded], axis=1)\n",
        "    df = df_ohe.drop(columns=['category', 'image_path'])\n",
        "    del df_ohe, df_encoded\n",
        "    gc.collect()\n",
        "    return df"
      ],
      "metadata": {
        "id": "x10jAk7IbteC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def split_df(df):\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "    return train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "Y00AjlsJbteC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(df, batch_size):\n",
        "    num_samples = len(df)\n",
        "    while True:\n",
        "        for offset in range(0, num_samples, batch_size):\n",
        "            batch_df = df.iloc[offset:offset+batch_size]\n",
        "\n",
        "            X_batch = np.array(batch_df['processed_images'].tolist())\n",
        "            Y_batch = batch_df.drop(columns=['processed_images']).values\n",
        "\n",
        "            yield X_batch, Y_batch"
      ],
      "metadata": {
        "id": "N_d5byZ5bteC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Testing"
      ],
      "metadata": {
        "id": "kPym2zehbteC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(train_df, val_df):\n",
        "    train_gen = data_generator(train_df, batch_size=32)\n",
        "    val_gen = data_generator(val_df, batch_size=32)\n",
        "    steps_per_epoch = len(train_df) // 32\n",
        "    validation_steps = len(val_df) // 32\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_gen,\n",
        "        validation_steps=validation_steps,\n",
        "        epochs=50,\n",
        "        callbacks=[early_stopping]\n",
        "    )"
      ],
      "metadata": {
        "id": "I1Vdt8r8bteC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def model_test(test_df):\n",
        "    target_names = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']\n",
        "    y_test = test_df[target_names].values\n",
        "    y_pred = model.predict(np.array(test_df['processed_images'].tolist()))\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "    y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred_classes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    report = classification_report(y_true, y_pred_classes, target_names=target_names)\n",
        "    print(report)"
      ],
      "metadata": {
        "id": "9T28TBDIbteD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data_train__test_and_save_model(df, model_name):\n",
        "    processed_image = []\n",
        "    for img in df['image_path']:\n",
        "        processed_image.append(image_processing(img))\n",
        "    df['processed_images'] = processed_image\n",
        "    del processed_image\n",
        "    gc.collect()\n",
        "\n",
        "    df = ohe(df)\n",
        "\n",
        "    train_df , val_df , test_df = split_df(df)\n",
        "    del df\n",
        "    gc.collect()\n",
        "\n",
        "    model_train(train_df , val_df)\n",
        "\n",
        "    model_test(test_df)\n",
        "\n",
        "    model.save(model_name)"
      ],
      "metadata": {
        "id": "SMRCPYw1bteD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "process_data_train__test_and_save_model(df, \"Multi_Class_Custom_Model.h5\")"
      ],
      "metadata": {
        "id": "HWRu_fA-bteD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3- Third Method: One VS Rest Approach Using Four Custom Binary Class Classification Models (Accuracy 81%)"
      ],
      "metadata": {
        "id": "Jum_bTIAbteD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset\"\n",
        "\n",
        "data = []\n",
        "\n",
        "for category in os.listdir(dataset_path):\n",
        "    category_path = os.path.join(dataset_path, category)\n",
        "\n",
        "    if os.path.isdir(category_path):\n",
        "        for image_name in os.listdir(category_path):\n",
        "            image_path = os.path.join(category_path, image_name)\n",
        "\n",
        "            data.append({\"image_path\": image_path, \"category\": category})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "output_csv = \"dataset.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"CSV file saved as {output_csv}\")\n"
      ],
      "metadata": {
        "id": "J5Q7COeebteD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Processing"
      ],
      "metadata": {
        "id": "1PsB_BrJbteE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_processing(img):\n",
        "    img = cv2.imread(img)\n",
        "\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    img_resized = cv2.resize(img_gray, (224,224))\n",
        "\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(7,7))\n",
        "    img_clahe = clahe.apply(img_resized)\n",
        "\n",
        "    biF = cv2.bilateralFilter(img_clahe, 5, 21, 21)\n",
        "\n",
        "    norm_img = cv2.normalize(biF, None, alpha=0.0, beta=1.0, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "\n",
        "    return norm_img"
      ],
      "metadata": {
        "id": "ByYtkBxsbteE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "processed_image = []\n",
        "for img in df['image_path']:\n",
        "    processed_image.append(image_processing(img))\n",
        "df['processed_image'] = processed_image\n",
        "df = df.drop(columns = ['image_path'])\n",
        "\n",
        "output_csv = \"dataset_processed.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "del processed_image\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "-sVA2SqEbteE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouping Classes"
      ],
      "metadata": {
        "id": "4qBaMr4WbteE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = df.groupby('category')\n",
        "for category, data in grouped:\n",
        "    globals()[f\"df_{category}\"] = data"
      ],
      "metadata": {
        "id": "xpNAjcd_bteF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model"
      ],
      "metadata": {
        "id": "7bRm4usYbteF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224,224,1)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "mh6hx7xHbteF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Data"
      ],
      "metadata": {
        "id": "7rIFeI-ObteF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(df, batch_size):\n",
        "    num_samples = len(df)\n",
        "    while True:\n",
        "        for offset in range(0, num_samples, batch_size):\n",
        "            batch_df = df.iloc[offset:offset+batch_size]\n",
        "\n",
        "            X_batch = np.array(batch_df['processed_image'].tolist())\n",
        "            Y_batch = np.array(batch_df['category'].tolist())\n",
        "\n",
        "            yield X_batch, Y_batch"
      ],
      "metadata": {
        "id": "xvojcxkAbteF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(train_df, val_df):\n",
        "    train_gen = data_generator(train_df, batch_size = 4)\n",
        "    val_gen = data_generator(val_df, batch_size = 4)\n",
        "\n",
        "    steps_per_epoch = len(train_df) // 4\n",
        "    validation_steps = len(val_df) // 4\n",
        "\n",
        "    model = create_model()\n",
        "    history = model.fit(\n",
        "        train_gen,\n",
        "        steps_per_epoch = steps_per_epoch,\n",
        "        validation_data = val_gen,\n",
        "        validation_steps = validation_steps,\n",
        "        epochs = 50,\n",
        "        callbacks = [EarlyStopping (\n",
        "        monitor = 'val_loss',\n",
        "        patience = 3,\n",
        "        restore_best_weights = True\n",
        "        )]\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "U4g8c1kBbteG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data__train_and_save_model(df, model_name):\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = model_train(train_df , test_df)\n",
        "\n",
        "    model.save(model_name)"
      ],
      "metadata": {
        "id": "lR_nj38lbteG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training and Testing"
      ],
      "metadata": {
        "id": "jmJHMj57bteH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, df_NonDemented_test = train_test_split(df_NonDemented, test_size=0.1, random_state=42)\n",
        "train_df['category'] = 1\n",
        "\n",
        "sampled_df1 = df_ModerateDemented.sample(n = len(train_df)  // 3, random_state=42)\n",
        "sampled_df2 = df_MildDemented.sample(n = len(train_df)  // 3, random_state=42)\n",
        "sampled_df3 = df_VeryMildDemented.sample(n = len(train_df)  // 3, random_state=42)\n",
        "\n",
        "sampled_df = pd.concat([sampled_df1, sampled_df2, sampled_df3], ignore_index=True)\n",
        "sampled_df['category'] = 0\n",
        "\n",
        "df_train = pd.concat([train_df, sampled_df], ignore_index=True)\n",
        "\n",
        "process_data__train_and_save_model(df_train, \"model_NonDemented.h5\")"
      ],
      "metadata": {
        "id": "YQXOoF0ObteH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, df_ModerateDemented_test = train_test_split(df_ModerateDemented, test_size=0.1, random_state=42)\n",
        "train_df['category'] = 1\n",
        "\n",
        "sampled_df1 = df_NonDemented.sample(n = len(train_df) // 3, random_state=42)\n",
        "sampled_df2 = df_MildDemented.sample(n = len(train_df) // 3, random_state=42)\n",
        "sampled_df3 = df_VeryMildDemented.sample(n = len(train_df) // 3, random_state=42)\n",
        "\n",
        "sampled_df = pd.concat([sampled_df1, sampled_df2, sampled_df3], ignore_index=True)\n",
        "sampled_df['category'] = 0\n",
        "\n",
        "df_train = pd.concat([train_df, sampled_df], ignore_index=True)\n",
        "\n",
        "process_data__train_and_save_model(df_train, \"model_ModerateDemented.h5\")"
      ],
      "metadata": {
        "id": "FJepVVT4bteH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, df_MildDemented_test = train_test_split(df_MildDemented, test_size=0.1, random_state=42)\n",
        "train_df['category'] = 1\n",
        "\n",
        "sampled_df1 = df_NonDemented.sample(n = len(train_df) // 3, random_state=42)\n",
        "sampled_df2 = df_ModerateDemented.sample(n = len(train_df) // 3, random_state=42)\n",
        "sampled_df3 = df_VeryMildDemented.sample(n = len(train_df) // 3, random_state=42)\n",
        "\n",
        "sampled_df = pd.concat([sampled_df1, sampled_df2, sampled_df3], ignore_index=True)\n",
        "sampled_df['category'] = 0\n",
        "\n",
        "df_train = pd.concat([train_df, sampled_df], ignore_index=True)\n",
        "\n",
        "process_data__train_and_save_model(df_train, \"model_MildDemented.h5\")"
      ],
      "metadata": {
        "id": "NbUJtZE2bteH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, df_VeryMildDemented_test = train_test_split(df_VeryMildDemented, test_size=0.1, random_state=42)\n",
        "train_df['category'] = 1\n",
        "\n",
        "sampled_df1 = df_NonDemented.sample(n = len(train_df) // 3, random_state=42)\n",
        "sampled_df2 = df_ModerateDemented.sample(n = len(train_df) // 3, random_state=42)\n",
        "sampled_df3 = df_MildDemented.sample(n = len(train_df) // 3, random_state=42)\n",
        "\n",
        "sampled_df = pd.concat([sampled_df1, sampled_df2, sampled_df3], ignore_index=True)\n",
        "sampled_df['category'] = 0\n",
        "\n",
        "df_train = pd.concat([train_df, sampled_df], ignore_index=True)\n",
        "\n",
        "process_data__train_and_save_model(df_train, \"model_VeryMildDemented.h5\")"
      ],
      "metadata": {
        "id": "g1we3bFqbteH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "NonDemented_model = load_model('/kaggle/working/model_NonDemented.h5')\n",
        "ModerateDemented_model = load_model('/kaggle/working/model_ModerateDemented.h5')\n",
        "MildDemented_model = load_model('/kaggle/working/model_MildDemented.h5')\n",
        "VeryMildDemented_model = load_model('/kaggle/working/model_VeryMildDemented.h5')\n",
        "\n",
        "target_names=[\"NonDemented\", \"ModerateDemented\", \"MildDemented\", \"VeryMildDemented\", \"NotSure\"]\n",
        "\n",
        "df = pd.concat([df_NonDemented_test, df_ModerateDemented_test, df_MildDemented_test, df_VeryMildDemented_test], ignore_index=True)\n",
        "\n",
        "Y_Predict = []\n",
        "Y_True = []\n",
        "\n",
        "for img, category in zip(df['processed_image'], df['category']):\n",
        "    if category == \"NonDemented\":\n",
        "        Y_True.append(0)\n",
        "    elif category == \"ModerateDemented\":\n",
        "        Y_True.append(1)\n",
        "    elif category == \"MildDemented\":\n",
        "        Y_True.append(2)\n",
        "    elif category == \"VeryMildDemented\":\n",
        "        Y_True.append(3)\n",
        "\n",
        "    predicted_label = 4\n",
        "\n",
        "    if NonDemented_model.predict(np.expand_dims(img, axis=0), verbose=0) >= 0.55:\n",
        "        predicted_label = 0\n",
        "    elif ModerateDemented_model.predict(np.expand_dims(img, axis=0), verbose=0) >= 0.75:\n",
        "        predicted_label = 1\n",
        "    elif MildDemented_model.predict(np.expand_dims(img, axis=0), verbose=0) >= 0.55:\n",
        "        predicted_label = 2\n",
        "    elif VeryMildDemented_model.predict(np.expand_dims(img, axis=0), verbose=0) >= 0.45:\n",
        "        predicted_label = 3\n",
        "\n",
        "    Y_Predict.append(predicted_label)\n",
        "\n",
        "Y_True = np.array(Y_True)\n",
        "Y_Predict = np.array(Y_Predict)\n",
        "\n",
        "cm = confusion_matrix(Y_True, Y_Predict)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels = target_names, yticklabels = target_names)\n",
        "plt.show()\n",
        "\n",
        "report = classification_report(Y_True, Y_Predict, target_names = target_names)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "ZVDvMC4SbteH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4- Fourth Method: One VS One Approach Using Six Custom Binary Class Classification Models (Accuracy 96%)"
      ],
      "metadata": {
        "id": "niVIe8-VbteI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = \"/kaggle/input/augmented-alzheimer-mri-dataset/AugmentedAlzheimerDataset\"\n",
        "\n",
        "data = []\n",
        "\n",
        "for category in os.listdir(dataset_path):\n",
        "    category_path = os.path.join(dataset_path, category)\n",
        "\n",
        "    if os.path.isdir(category_path):\n",
        "        for image_name in os.listdir(category_path):\n",
        "            image_path = os.path.join(category_path, image_name)\n",
        "\n",
        "            data.append({\"image_path\": image_path, \"category\": category})\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "output_csv = \"dataset.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"CSV file saved as {output_csv}\")\n"
      ],
      "metadata": {
        "id": "CEmPUUfNbteI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Processing"
      ],
      "metadata": {
        "id": "izozWJrrbteI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def image_processing(img):\n",
        "    img = cv2.imread(img)\n",
        "\n",
        "    img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    img_resized = cv2.resize(img_gray, (224,224))\n",
        "\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(7,7))\n",
        "    img_clahe = clahe.apply(img_resized)\n",
        "\n",
        "    norm_img = cv2.normalize(img_clahe, None, alpha=0.0, beta=1.0, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "\n",
        "    return norm_img"
      ],
      "metadata": {
        "id": "JWgevsK7bteI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "processed_image = []\n",
        "for img in df['image_path']:\n",
        "    processed_image.append(image_processing(img))\n",
        "df['processed_image'] = processed_image\n",
        "df = df.drop(columns = ['image_path'])\n",
        "\n",
        "output_csv = \"dataset_processed.csv\"\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "del processed_image\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "jNkuCz9nbteI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouping Data"
      ],
      "metadata": {
        "id": "kVXd7HB-bteI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grouped = df.groupby('category')\n",
        "for category, data in grouped:\n",
        "    globals()[f\"df_{category}\"] = data"
      ],
      "metadata": {
        "id": "-PyZmig-bteI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Models"
      ],
      "metadata": {
        "id": "LQPyCOXkbteJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(add_more_layers):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224,224,1)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    if add_more_layers:\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(64, activation='relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "pPPvMt6EbteJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_Deep_model(add_more_layers):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 1)))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    if add_more_layers:\n",
        "        model.add(Conv2D(256, (3, 3), activation='relu'))\n",
        "        model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Dropout(0.3))\n",
        "    if add_more_layers:\n",
        "        model.add(Dense(32, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer = Adam(learning_rate=0.0001), loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "HnseGO_TbteJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure Data"
      ],
      "metadata": {
        "id": "jp-qYjo-bteJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(df, batch_size):\n",
        "    num_samples = len(df)\n",
        "    while True:\n",
        "        for offset in range(0, num_samples, batch_size):\n",
        "            batch_df = df.iloc[offset:offset+batch_size]\n",
        "\n",
        "            X_batch = np.array(batch_df['processed_image'].tolist())\n",
        "            Y_batch = np.array(batch_df['category'].tolist())\n",
        "\n",
        "            yield X_batch, Y_batch"
      ],
      "metadata": {
        "id": "wsUjpfnRbteJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def model_train(train_df, val_df, add_more_layers, patience, use_deep_model):\n",
        "    train_gen = data_generator(train_df, batch_size = 4)\n",
        "    val_gen = data_generator(val_df, batch_size = 4)\n",
        "\n",
        "    steps_per_epoch = len(train_df) // 4\n",
        "    validation_steps = len(val_df) // 4\n",
        "\n",
        "    model = -1\n",
        "\n",
        "    if use_deep_model:\n",
        "        model = create_Deep_model(add_more_layers)\n",
        "    else:\n",
        "        model = create_model(add_more_layers)\n",
        "\n",
        "    model_history = model.fit(\n",
        "        train_gen,\n",
        "        steps_per_epoch = steps_per_epoch,\n",
        "        validation_data = val_gen,\n",
        "        validation_steps = validation_steps,\n",
        "        epochs = 50,\n",
        "        callbacks = [EarlyStopping (\n",
        "        monitor = 'val_loss',\n",
        "        patience = patience,\n",
        "        restore_best_weights = True\n",
        "        )]\n",
        "    )\n",
        "\n",
        "    return model, model_history"
      ],
      "metadata": {
        "id": "rRlqE0IKbteJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_curves(history):\n",
        "    train_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    train_acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    ax1.plot(train_loss, label='Training Loss', color='blue')\n",
        "    ax1.plot(val_loss, label='Validation Loss', color='orange')\n",
        "    ax1.set_title('Loss Curve')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(train_acc, label='Training Accuracy', color='green')\n",
        "    ax2.plot(val_acc, label='Validation Accuracy', color='red')\n",
        "    ax2.set_title('Accuracy Curve')\n",
        "    ax2.set_xlabel('Epochs')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "5za1wrribteJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data__train_and_save_model(df, model_name, add_more_layers, patience, use_deep_model):\n",
        "    df = df.sample(frac=1).reset_index(drop=True)\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    model, model_history = model_train(train_df , test_df, add_more_layers, patience, use_deep_model)\n",
        "\n",
        "    plot_curves(model_history)\n",
        "\n",
        "    model.save(model_name)"
      ],
      "metadata": {
        "id": "8JSjzfFsbteK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Training"
      ],
      "metadata": {
        "id": "wQSbNllhbteK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, df_NonDemented_test = train_test_split(df_NonDemented, test_size=0.2, random_state=42)\n",
        "train_df['category'] = 1"
      ],
      "metadata": {
        "id": "vcqj1yFybteK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = df_VeryMildDemented.sample(n = len(train_df), random_state=42)\n",
        "sampled_df['category'] = 0\n",
        "\n",
        "df_train = pd.concat([train_df, sampled_df], ignore_index=True)\n",
        "\n",
        "process_data__train_and_save_model(df_train, \"model_NonVSVeryMild.h5\", True, 5, True)"
      ],
      "metadata": {
        "id": "IjGRKlaSbteK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = df_MildDemented.sample(n = len(train_df), random_state=42)\n",
        "sampled_df['category'] = 0\n",
        "\n",
        "df_train = pd.concat([train_df, sampled_df], ignore_index=True)\n",
        "\n",
        "process_data__train_and_save_model(df_train, \"model_NonVSMild.h5\", True, 5, True)"
      ],
      "metadata": {
        "id": "Z0APFdZSbteK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = df_ModerateDemented.copy()\n",
        "sampled_df['category'] = 0\n",
        "\n",
        "df_train = pd.concat([train_df, sampled_df], ignore_index=True)\n",
        "\n",
        "process_data__train_and_save_model(df_train, \"model_NonVSModerate.h5\", False, 2, False)"
      ],
      "metadata": {
        "id": "QswOPmzrbteK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, df_VeryMildDemented_test = train_test_split(df_VeryMildDemented, test_size=0.2, random_state=42)\n",
        "train_df['category'] = 1"
      ],
      "metadata": {
        "id": "I1-_Bq4xbteL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = df_MildDemented.sample(n = len(train_df), random_state=42)\n",
        "sampled_df['category'] = 0\n",
        "\n",
        "df_train = pd.concat([train_df, sampled_df], ignore_index=True)\n",
        "\n",
        "process_data__train_and_save_model(df_train, \"model_VeryMildVSMild.h5\", True, 5, True)"
      ],
      "metadata": {
        "id": "71uvRbSNbteL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = df_ModerateDemented.copy()\n",
        "sampled_df['category'] = 0\n",
        "\n",
        "df_train = pd.concat([train_df, sampled_df], ignore_index=True)\n",
        "\n",
        "process_data__train_and_save_model(df_train, \"model_VeryMildVSModerate.h5\", False, 2, False)"
      ],
      "metadata": {
        "id": "q9Z9LcUJbteQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, df_MildDemented_test = train_test_split(df_MildDemented, test_size=0.2, random_state=42)\n",
        "train_df['category'] = 1"
      ],
      "metadata": {
        "id": "2nMcliC5bteQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_df = df_ModerateDemented.copy()\n",
        "sampled_df['category'] = 0\n",
        "\n",
        "df_train = pd.concat([train_df, sampled_df], ignore_index=True)\n",
        "\n",
        "process_data__train_and_save_model(df_train, \"model_MildVSModerate.h5\", True, 2, False)"
      ],
      "metadata": {
        "id": "BdRgIFtabteR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, df_ModerateDemented_test = train_test_split(df_ModerateDemented, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "bZYvpxg-bteR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models Final Testing"
      ],
      "metadata": {
        "id": "N9miWBrQbteR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.concat([df_NonDemented_test, df_VeryMildDemented_test, df_MildDemented_test, df_ModerateDemented_test], ignore_index=True)\n",
        "\n",
        "df = df.sample(n = len(df) //10).reset_index(drop=True)"
      ],
      "metadata": {
        "id": "4EVMuOIFbteR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Y_True = []\n",
        "\n",
        "for category in df['category']:\n",
        "    if category == \"NonDemented\":\n",
        "        Y_True.append(0)\n",
        "    elif category == \"VeryMildDemented\":\n",
        "        Y_True.append(1)\n",
        "    elif category == \"MildDemented\":\n",
        "        Y_True.append(2)\n",
        "    elif category == \"ModerateDemented\":\n",
        "        Y_True.append(3)"
      ],
      "metadata": {
        "id": "mI8CgfJZbteR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "NonVSVeryMild_model = load_model('model_NonVSVeryMild.h5')\n",
        "NonVSMild_model = load_model('model_NonVSMild.h5')\n",
        "NonVSModerate_model = load_model('model_NonVSModerate.h5')\n",
        "\n",
        "VeryMildVSMild_model = load_model('model_VeryMildVSMild.h5')\n",
        "VeryMildVSModerate_model = load_model('model_VeryMildVSModerate.h5')\n",
        "\n",
        "MildVSModerate_model = load_model('model_MildVSModerate.h5')\n",
        "\n",
        "confidence = {\n",
        "    'NonVSVeryMild': 0.9264,\n",
        "    'NonVSMild': 0.9684,\n",
        "    'NonVSModerate': 0.9947,\n",
        "    'VeryMildVSMild': 0.9449,\n",
        "    'VeryMildVSModerate': 0.9952,\n",
        "    'MildVSModerate': 0.9820\n",
        "}"
      ],
      "metadata": {
        "id": "WOSJ5X_1bteR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_thresholds(img):\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "\n",
        "    NonVSVeryMild_Predict = float(NonVSVeryMild_model.predict(img, verbose=0))\n",
        "    NonVSMild_Predict = float(NonVSMild_model.predict(img, verbose=0))\n",
        "    NonVSModerate_Predict = float(NonVSModerate_model.predict(img, verbose=0))\n",
        "\n",
        "    VeryMildVSMild_Predict = float(VeryMildVSMild_model.predict(img, verbose=0))\n",
        "    VeryMildVSModerate_Predict = float(VeryMildVSModerate_model.predict(img, verbose=0))\n",
        "\n",
        "    MildVSModerate_Predict = float(MildVSModerate_model.predict(img, verbose=0))\n",
        "\n",
        "    predictionProbabilities = [0, 0, 0, 0]\n",
        "    predictionProbabilities[0] = ((NonVSVeryMild_Predict * confidence['NonVSVeryMild']) + (NonVSMild_Predict * confidence['NonVSMild']) + (NonVSModerate_Predict * confidence['NonVSModerate'])) / 3\n",
        "    predictionProbabilities[1] = (((1 - NonVSVeryMild_Predict) * confidence['NonVSVeryMild']) + (VeryMildVSMild_Predict * confidence['VeryMildVSMild']) + (VeryMildVSModerate_Predict * confidence['VeryMildVSModerate'])) / 3\n",
        "    predictionProbabilities[2] = (((1 - NonVSMild_Predict) * confidence['NonVSMild']) + ((1 - VeryMildVSMild_Predict) * confidence['VeryMildVSMild']) + (MildVSModerate_Predict * confidence['MildVSModerate'])) / 3\n",
        "    predictionProbabilities[3] = (((1 - NonVSModerate_Predict) * confidence['NonVSModerate']) + ((1 - VeryMildVSModerate_Predict) * confidence['VeryMildVSModerate']) + ((1 - MildVSModerate_Predict) * confidence['MildVSModerate'])) / 3\n",
        "\n",
        "    maxProb = np.argmax(predictionProbabilities)\n",
        "    if predictionProbabilities[maxProb] >= 0.8:\n",
        "        return maxProb\n",
        "    else:\n",
        "        return 4"
      ],
      "metadata": {
        "id": "8rRib44DbteR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "Y_Predict = []\n",
        "for img in df['processed_image']:\n",
        "    Y_Predict.append(predict_with_thresholds(img))\n",
        "\n",
        "Y_True = np.array(Y_True)\n",
        "Y_Predict = np.array(Y_Predict)"
      ],
      "metadata": {
        "id": "pmPxMjfabteR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "target_names=[\"NonDemented\", \"VeryMildDemented\", \"MildDemented\", \"ModerateDemented\", \"Unsure\"]"
      ],
      "metadata": {
        "id": "vupE2SLwbteS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(Y_True, Y_Predict)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels = target_names, yticklabels = target_names)\n",
        "plt.show()\n",
        "\n",
        "report = classification_report(Y_True, Y_Predict, target_names = target_names)\n",
        "print(report)"
      ],
      "metadata": {
        "id": "iS6FUvdgbteS"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}